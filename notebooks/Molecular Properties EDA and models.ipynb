{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "This kernel is created using data of `Predicting Molecular Properties` competition.\n",
    "We have information about atom couples in molecules and need to predict `scalar_coupling_constant` between these atoms.\n",
    "\n",
    "![](http://www.et.byu.edu/~rowley/VLEfinal/methane_dimer.gif)\n",
    "\n",
    "In this kernel I'll do EDA and will try some approaches to modelling.\n",
    "\n",
    "~Thanks to the new kaggle update we can write code in kernels and import it. This is much more convenient and useful. I'm moving all the functions I can into this script: https://www.kaggle.com/artgor/artgor-utils So if you see somewhere code like artgot_utils.function_name(parameters) - it is from this script~\n",
    "**Important**:\n",
    "It seems that after forking the utility script isn't copied, so I have decided to move all the functions back into the kernel. But utility kernel can't be removed from the kernel... I hope I was able to deal with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install -U vega_datasets notebook vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>    requirejs.config({\n",
       "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
       "        paths: {'vega': 'https://cdn.jsdelivr.net/npm/vega@v5.4.0?noext', 'vega-lib': 'https://cdn.jsdelivr.net/npm/vega-lib?noext', 'vega-lite': 'https://cdn.jsdelivr.net/npm/vega-lite@v3.3.0?noext', 'vega-embed': 'https://cdn.jsdelivr.net/npm/vega-embed@3?noext'}\n",
       "    });\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import altair as alt\n",
    "from altair.vega import v5\n",
    "from IPython.display import HTML\n",
    "\n",
    "# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "def prepare_altair():\n",
    "    \"\"\"\n",
    "    Helper function to prepare altair for working.\n",
    "    \"\"\"\n",
    "\n",
    "    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n",
    "    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "    noext = \"?noext\"\n",
    "    \n",
    "    paths = {\n",
    "        'vega': vega_url + noext,\n",
    "        'vega-lib': vega_lib_url + noext,\n",
    "        'vega-lite': vega_lite_url + noext,\n",
    "        'vega-embed': vega_embed_url + noext\n",
    "    }\n",
    "    \n",
    "    workaround = f\"\"\"    requirejs.config({{\n",
    "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "        paths: {paths}\n",
    "    }});\n",
    "    \"\"\"\n",
    "    \n",
    "    return workaround\n",
    "    \n",
    "\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "           \n",
    "\n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot altair visualizations.\n",
    "    \"\"\"\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float64).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "\n",
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    :params: verbose - parameters for gradient boosting models\n",
    "    :params: early_stopping_rounds - parameters for gradient boosting models\n",
    "    :params: n_estimators - parameters for gradient boosting models\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "\n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of classification models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    :params: verbose - parameters for gradient boosting models\n",
    "    :params: early_stopping_rounds - parameters for gradient boosting models\n",
    "    :params: n_estimators - parameters for gradient boosting models\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns == None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros((len(X), len(set(y.values))))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros((len(X_test), oof.shape[1]))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "# setting up altair\n",
    "workaround = prepare_altair()\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround,\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalar_coupling_contributions.csv',\n",
       " '.DS_Store',\n",
       " 'mulliken_charges.csv',\n",
       " 'structures.csv',\n",
       " 'test.csv',\n",
       " '.gitkeep',\n",
       " 'structures.zip',\n",
       " 'train.csv',\n",
       " 'magnetic_shielding_tensors.csv',\n",
       " 'test.feather',\n",
       " 'dipole_moments.csv',\n",
       " 'train.feather',\n",
       " 'sample_submission.csv',\n",
       " 'potential_energy.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "file_folder = '../input/champs-scalar-coupling' if 'champs-scalar-coupling' in os.listdir('../data/input/') else '../data/input'\n",
    "os.listdir(file_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of files, let's focus on the main ones for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(f'{file_folder}/train.csv')\n",
    "test = pd.read_csv(f'{file_folder}/test.csv')\n",
    "sub = pd.read_csv(f'{file_folder}/sample_submission.csv')\n",
    "structures = pd.read_csv(f'{file_folder}/structures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1768546</th>\n",
       "      <td>1768546</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768547</th>\n",
       "      <td>1768547</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-0.979190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768548</th>\n",
       "      <td>1768548</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.084070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768549</th>\n",
       "      <td>1768549</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>9.316850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768550</th>\n",
       "      <td>1768550</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>0.662348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768551</th>\n",
       "      <td>1768551</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768552</th>\n",
       "      <td>1768552</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768553</th>\n",
       "      <td>1768553</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768554</th>\n",
       "      <td>1768554</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.301320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768555</th>\n",
       "      <td>1768555</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>7.934060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768556</th>\n",
       "      <td>1768556</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.991230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768557</th>\n",
       "      <td>1768557</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.491420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768558</th>\n",
       "      <td>1768558</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768559</th>\n",
       "      <td>1768559</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768560</th>\n",
       "      <td>1768560</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.531550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768561</th>\n",
       "      <td>1768561</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.915240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768562</th>\n",
       "      <td>1768562</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>3.119310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768563</th>\n",
       "      <td>1768563</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>7.111670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768564</th>\n",
       "      <td>1768564</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768565</th>\n",
       "      <td>1768565</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.928260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768566</th>\n",
       "      <td>1768566</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768567</th>\n",
       "      <td>1768567</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>3.436730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768568</th>\n",
       "      <td>1768568</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>6.661390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768569</th>\n",
       "      <td>1768569</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768570</th>\n",
       "      <td>1768570</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768571</th>\n",
       "      <td>1768571</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>8.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768572</th>\n",
       "      <td>1768572</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768573</th>\n",
       "      <td>1768573</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>81.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768574</th>\n",
       "      <td>1768574</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.614970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768575</th>\n",
       "      <td>1768575</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.923460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768643</th>\n",
       "      <td>1768643</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768644</th>\n",
       "      <td>1768644</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>8.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768645</th>\n",
       "      <td>1768645</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-0.886784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768646</th>\n",
       "      <td>1768646</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.739170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768647</th>\n",
       "      <td>1768647</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768648</th>\n",
       "      <td>1768648</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.125320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768649</th>\n",
       "      <td>1768649</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768650</th>\n",
       "      <td>1768650</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.797670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768651</th>\n",
       "      <td>1768651</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.363890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768652</th>\n",
       "      <td>1768652</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.879070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768653</th>\n",
       "      <td>1768653</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>81.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768654</th>\n",
       "      <td>1768654</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>8.428440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768655</th>\n",
       "      <td>1768655</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>0.547185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768656</th>\n",
       "      <td>1768656</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.097470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768657</th>\n",
       "      <td>1768657</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>2.416950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768658</th>\n",
       "      <td>1768658</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>9.273210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768659</th>\n",
       "      <td>1768659</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>81.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768660</th>\n",
       "      <td>1768660</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768661</th>\n",
       "      <td>1768661</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-10.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768662</th>\n",
       "      <td>1768662</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>6.661310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768663</th>\n",
       "      <td>1768663</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.928260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768664</th>\n",
       "      <td>1768664</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768665</th>\n",
       "      <td>1768665</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>3.436910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768666</th>\n",
       "      <td>1768666</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>80.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768667</th>\n",
       "      <td>1768667</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768668</th>\n",
       "      <td>1768668</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.923630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768669</th>\n",
       "      <td>1768669</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.141230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768670</th>\n",
       "      <td>1768670</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>8.388730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768671</th>\n",
       "      <td>1768671</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>1.614850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768672</th>\n",
       "      <td>1768672</td>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>81.529700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "1768546  1768546  dsgdb9nsd_057518             9             0  1JHC   \n",
       "1768547  1768547  dsgdb9nsd_057518             9             1  2JHC   \n",
       "1768548  1768548  dsgdb9nsd_057518             9             2  3JHC   \n",
       "1768549  1768549  dsgdb9nsd_057518             9             3  3JHC   \n",
       "1768550  1768550  dsgdb9nsd_057518             9             4  3JHC   \n",
       "1768551  1768551  dsgdb9nsd_057518             9            10  2JHH   \n",
       "1768552  1768552  dsgdb9nsd_057518             9            11  2JHH   \n",
       "1768553  1768553  dsgdb9nsd_057518            10             0  1JHC   \n",
       "1768554  1768554  dsgdb9nsd_057518            10             1  2JHC   \n",
       "1768555  1768555  dsgdb9nsd_057518            10             2  3JHC   \n",
       "1768556  1768556  dsgdb9nsd_057518            10             3  3JHC   \n",
       "1768557  1768557  dsgdb9nsd_057518            10             4  3JHC   \n",
       "1768558  1768558  dsgdb9nsd_057518            10            11  2JHH   \n",
       "1768559  1768559  dsgdb9nsd_057518            11             0  1JHC   \n",
       "1768560  1768560  dsgdb9nsd_057518            11             1  2JHC   \n",
       "1768561  1768561  dsgdb9nsd_057518            11             2  3JHC   \n",
       "1768562  1768562  dsgdb9nsd_057518            11             3  3JHC   \n",
       "1768563  1768563  dsgdb9nsd_057518            11             4  3JHC   \n",
       "1768564  1768564  dsgdb9nsd_057518            12             0  3JHC   \n",
       "1768565  1768565  dsgdb9nsd_057518            12             1  2JHC   \n",
       "1768566  1768566  dsgdb9nsd_057518            12             2  1JHC   \n",
       "1768567  1768567  dsgdb9nsd_057518            12             3  3JHC   \n",
       "1768568  1768568  dsgdb9nsd_057518            12             4  3JHC   \n",
       "1768569  1768569  dsgdb9nsd_057518            12            13  2JHH   \n",
       "1768570  1768570  dsgdb9nsd_057518            12            14  2JHH   \n",
       "1768571  1768571  dsgdb9nsd_057518            13             0  3JHC   \n",
       "1768572  1768572  dsgdb9nsd_057518            13             1  2JHC   \n",
       "1768573  1768573  dsgdb9nsd_057518            13             2  1JHC   \n",
       "1768574  1768574  dsgdb9nsd_057518            13             3  3JHC   \n",
       "1768575  1768575  dsgdb9nsd_057518            13             4  3JHC   \n",
       "...          ...               ...           ...           ...   ...   \n",
       "1768643  1768643  dsgdb9nsd_057518            23            25  2JHH   \n",
       "1768644  1768644  dsgdb9nsd_057518            24             4  3JHC   \n",
       "1768645  1768645  dsgdb9nsd_057518            24             5  2JHC   \n",
       "1768646  1768646  dsgdb9nsd_057518            24             6  3JHC   \n",
       "1768647  1768647  dsgdb9nsd_057518            24             7  1JHC   \n",
       "1768648  1768648  dsgdb9nsd_057518            24             8  3JHC   \n",
       "1768649  1768649  dsgdb9nsd_057518            24            25  2JHH   \n",
       "1768650  1768650  dsgdb9nsd_057518            25             4  3JHC   \n",
       "1768651  1768651  dsgdb9nsd_057518            25             5  2JHC   \n",
       "1768652  1768652  dsgdb9nsd_057518            25             6  3JHC   \n",
       "1768653  1768653  dsgdb9nsd_057518            25             7  1JHC   \n",
       "1768654  1768654  dsgdb9nsd_057518            25             8  3JHC   \n",
       "1768655  1768655  dsgdb9nsd_057518            26             4  3JHC   \n",
       "1768656  1768656  dsgdb9nsd_057518            26             5  2JHC   \n",
       "1768657  1768657  dsgdb9nsd_057518            26             6  3JHC   \n",
       "1768658  1768658  dsgdb9nsd_057518            26             7  3JHC   \n",
       "1768659  1768659  dsgdb9nsd_057518            26             8  1JHC   \n",
       "1768660  1768660  dsgdb9nsd_057518            26            27  2JHH   \n",
       "1768661  1768661  dsgdb9nsd_057518            26            28  2JHH   \n",
       "1768662  1768662  dsgdb9nsd_057518            27             4  3JHC   \n",
       "1768663  1768663  dsgdb9nsd_057518            27             5  2JHC   \n",
       "1768664  1768664  dsgdb9nsd_057518            27             6  3JHC   \n",
       "1768665  1768665  dsgdb9nsd_057518            27             7  3JHC   \n",
       "1768666  1768666  dsgdb9nsd_057518            27             8  1JHC   \n",
       "1768667  1768667  dsgdb9nsd_057518            27            28  2JHH   \n",
       "1768668  1768668  dsgdb9nsd_057518            28             4  3JHC   \n",
       "1768669  1768669  dsgdb9nsd_057518            28             5  2JHC   \n",
       "1768670  1768670  dsgdb9nsd_057518            28             6  3JHC   \n",
       "1768671  1768671  dsgdb9nsd_057518            28             7  3JHC   \n",
       "1768672  1768672  dsgdb9nsd_057518            28             8  1JHC   \n",
       "\n",
       "         scalar_coupling_constant  \n",
       "1768546                 80.570600  \n",
       "1768547                 -0.979190  \n",
       "1768548                  2.084070  \n",
       "1768549                  9.316850  \n",
       "1768550                  0.662348  \n",
       "1768551                -11.083900  \n",
       "1768552                -11.437900  \n",
       "1768553                 80.083900  \n",
       "1768554                 -1.301320  \n",
       "1768555                  7.934060  \n",
       "1768556                  1.991230  \n",
       "1768557                  2.491420  \n",
       "1768558                -11.516700  \n",
       "1768559                 80.632100  \n",
       "1768560                 -1.531550  \n",
       "1768561                  1.915240  \n",
       "1768562                  3.119310  \n",
       "1768563                  7.111670  \n",
       "1768564                  1.698300  \n",
       "1768565                 -1.928260  \n",
       "1768566                 80.669100  \n",
       "1768567                  3.436730  \n",
       "1768568                  6.661390  \n",
       "1768569                -11.367700  \n",
       "1768570                -11.427800  \n",
       "1768571                  8.388800  \n",
       "1768572                 -1.141200  \n",
       "1768573                 81.528900  \n",
       "1768574                  1.614970  \n",
       "1768575                  1.923460  \n",
       "...                           ...  \n",
       "1768643                -11.074200  \n",
       "1768644                  8.256300  \n",
       "1768645                 -0.886784  \n",
       "1768646                  2.739170  \n",
       "1768647                 80.894700  \n",
       "1768648                  2.125320  \n",
       "1768649                -11.123000  \n",
       "1768650                  2.797670  \n",
       "1768651                 -1.363890  \n",
       "1768652                  1.879070  \n",
       "1768653                 81.542500  \n",
       "1768654                  8.428440  \n",
       "1768655                  0.547185  \n",
       "1768656                 -1.097470  \n",
       "1768657                  2.416950  \n",
       "1768658                  9.273210  \n",
       "1768659                 81.682000  \n",
       "1768660                -11.427700  \n",
       "1768661                -10.726300  \n",
       "1768662                  6.661310  \n",
       "1768663                 -1.928260  \n",
       "1768664                  1.698100  \n",
       "1768665                  3.436910  \n",
       "1768666                 80.668400  \n",
       "1768667                -11.367600  \n",
       "1768668                  1.923630  \n",
       "1768669                 -1.141230  \n",
       "1768670                  8.388730  \n",
       "1768671                  1.614850  \n",
       "1768672                 81.529700  \n",
       "\n",
       "[127 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"molecule_name\"]==\"dsgdb9nsd_057518\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934680</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.112281</td>\n",
       "      <td>1.012150</td>\n",
       "      <td>-1.464877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934681</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.898968</td>\n",
       "      <td>-0.413609</td>\n",
       "      <td>-0.915999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934682</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.560748</td>\n",
       "      <td>-0.335320</td>\n",
       "      <td>0.583859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934683</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>-2.238140</td>\n",
       "      <td>-1.170057</td>\n",
       "      <td>-1.063526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934684</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>0.142910</td>\n",
       "      <td>-1.213009</td>\n",
       "      <td>-1.758746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934685</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>1.688276</td>\n",
       "      <td>-1.007108</td>\n",
       "      <td>-1.693817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934686</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "      <td>2.300779</td>\n",
       "      <td>-1.635872</td>\n",
       "      <td>-0.425333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934687</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>7</td>\n",
       "      <td>C</td>\n",
       "      <td>2.270947</td>\n",
       "      <td>-1.761101</td>\n",
       "      <td>-2.910102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934688</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>8</td>\n",
       "      <td>C</td>\n",
       "      <td>2.126677</td>\n",
       "      <td>0.465289</td>\n",
       "      <td>-1.793553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934689</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>9</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.231114</td>\n",
       "      <td>1.645414</td>\n",
       "      <td>-1.347431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934690</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>10</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.369076</td>\n",
       "      <td>0.989219</td>\n",
       "      <td>-2.529906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934691</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>11</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.939324</td>\n",
       "      <td>1.501245</td>\n",
       "      <td>-0.937675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934692</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>12</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.396589</td>\n",
       "      <td>0.111767</td>\n",
       "      <td>1.134338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934693</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>13</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.380196</td>\n",
       "      <td>-1.328204</td>\n",
       "      <td>1.008574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934694</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>14</td>\n",
       "      <td>H</td>\n",
       "      <td>0.320537</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>0.777698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934695</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>15</td>\n",
       "      <td>H</td>\n",
       "      <td>-2.518404</td>\n",
       "      <td>-1.278086</td>\n",
       "      <td>-2.117276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934696</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>16</td>\n",
       "      <td>H</td>\n",
       "      <td>-2.177910</td>\n",
       "      <td>-2.174200</td>\n",
       "      <td>-0.629605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934697</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>17</td>\n",
       "      <td>H</td>\n",
       "      <td>-3.047629</td>\n",
       "      <td>-0.633737</td>\n",
       "      <td>-0.556307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934698</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>18</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.024097</td>\n",
       "      <td>-2.278818</td>\n",
       "      <td>-1.549920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934699</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>19</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.155951</td>\n",
       "      <td>-1.077377</td>\n",
       "      <td>-2.807428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934700</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>20</td>\n",
       "      <td>H</td>\n",
       "      <td>2.017916</td>\n",
       "      <td>-2.690514</td>\n",
       "      <td>-0.333266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934701</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>21</td>\n",
       "      <td>H</td>\n",
       "      <td>3.394905</td>\n",
       "      <td>-1.591665</td>\n",
       "      <td>-0.471041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934702</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>22</td>\n",
       "      <td>H</td>\n",
       "      <td>1.991996</td>\n",
       "      <td>-1.126298</td>\n",
       "      <td>0.489138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934703</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>23</td>\n",
       "      <td>H</td>\n",
       "      <td>1.928459</td>\n",
       "      <td>-1.320513</td>\n",
       "      <td>-3.852865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934704</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>24</td>\n",
       "      <td>H</td>\n",
       "      <td>3.365931</td>\n",
       "      <td>-1.727390</td>\n",
       "      <td>-2.902636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934705</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>25</td>\n",
       "      <td>H</td>\n",
       "      <td>1.969503</td>\n",
       "      <td>-2.814530</td>\n",
       "      <td>-2.905052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934706</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>26</td>\n",
       "      <td>H</td>\n",
       "      <td>1.843761</td>\n",
       "      <td>1.042205</td>\n",
       "      <td>-0.908679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934707</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>27</td>\n",
       "      <td>H</td>\n",
       "      <td>3.217285</td>\n",
       "      <td>0.526908</td>\n",
       "      <td>-1.884767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934708</th>\n",
       "      <td>dsgdb9nsd_057518</td>\n",
       "      <td>28</td>\n",
       "      <td>H</td>\n",
       "      <td>1.694478</td>\n",
       "      <td>0.955795</td>\n",
       "      <td>-2.671852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           molecule_name  atom_index atom         x         y         z\n",
       "934680  dsgdb9nsd_057518           0    C -1.112281  1.012150 -1.464877\n",
       "934681  dsgdb9nsd_057518           1    C -0.898968 -0.413609 -0.915999\n",
       "934682  dsgdb9nsd_057518           2    C -0.560748 -0.335320  0.583859\n",
       "934683  dsgdb9nsd_057518           3    C -2.238140 -1.170057 -1.063526\n",
       "934684  dsgdb9nsd_057518           4    C  0.142910 -1.213009 -1.758746\n",
       "934685  dsgdb9nsd_057518           5    C  1.688276 -1.007108 -1.693817\n",
       "934686  dsgdb9nsd_057518           6    C  2.300779 -1.635872 -0.425333\n",
       "934687  dsgdb9nsd_057518           7    C  2.270947 -1.761101 -2.910102\n",
       "934688  dsgdb9nsd_057518           8    C  2.126677  0.465289 -1.793553\n",
       "934689  dsgdb9nsd_057518           9    H -0.231114  1.645414 -1.347431\n",
       "934690  dsgdb9nsd_057518          10    H -1.369076  0.989219 -2.529906\n",
       "934691  dsgdb9nsd_057518          11    H -1.939324  1.501245 -0.937675\n",
       "934692  dsgdb9nsd_057518          12    H -1.396589  0.111767  1.134338\n",
       "934693  dsgdb9nsd_057518          13    H -0.380196 -1.328204  1.008574\n",
       "934694  dsgdb9nsd_057518          14    H  0.320537  0.282447  0.777698\n",
       "934695  dsgdb9nsd_057518          15    H -2.518404 -1.278086 -2.117276\n",
       "934696  dsgdb9nsd_057518          16    H -2.177910 -2.174200 -0.629605\n",
       "934697  dsgdb9nsd_057518          17    H -3.047629 -0.633737 -0.556307\n",
       "934698  dsgdb9nsd_057518          18    H -0.024097 -2.278818 -1.549920\n",
       "934699  dsgdb9nsd_057518          19    H -0.155951 -1.077377 -2.807428\n",
       "934700  dsgdb9nsd_057518          20    H  2.017916 -2.690514 -0.333266\n",
       "934701  dsgdb9nsd_057518          21    H  3.394905 -1.591665 -0.471041\n",
       "934702  dsgdb9nsd_057518          22    H  1.991996 -1.126298  0.489138\n",
       "934703  dsgdb9nsd_057518          23    H  1.928459 -1.320513 -3.852865\n",
       "934704  dsgdb9nsd_057518          24    H  3.365931 -1.727390 -2.902636\n",
       "934705  dsgdb9nsd_057518          25    H  1.969503 -2.814530 -2.905052\n",
       "934706  dsgdb9nsd_057518          26    H  1.843761  1.042205 -0.908679\n",
       "934707  dsgdb9nsd_057518          27    H  3.217285  0.526908 -1.884767\n",
       "934708  dsgdb9nsd_057518          28    H  1.694478  0.955795 -2.671852"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures[structures[\"molecule_name\"]==\"dsgdb9nsd_057518\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4658147 rows in train data.\n",
      "There are 2505542 rows in test data.\n",
      "There are 85003 distinct molecules in train data.\n",
      "There are 45772 distinct molecules in test data.\n",
      "There are 5 unique atoms.\n",
      "There are 8 unique types.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {train.shape[0]} rows in train data.')\n",
    "print(f'There are {test.shape[0]} rows in test data.')\n",
    "\n",
    "print(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\n",
    "print(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\n",
    "print(f\"There are {structures['atom'].nunique()} unique atoms.\")\n",
    "print(f\"There are {train['type'].nunique()} unique types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in out main data files we have information about moleculas and pairs of atoms\n",
    "- test set in ~2 times smaller that train set;\n",
    "- we have 28 unique atoms types and 8 coupling types;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.658147e+06</td>\n",
       "      <td>4.658147e+06</td>\n",
       "      <td>4.658147e+06</td>\n",
       "      <td>4.658147e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.329073e+06</td>\n",
       "      <td>1.335689e+01</td>\n",
       "      <td>5.883966e+00</td>\n",
       "      <td>1.592165e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.344691e+06</td>\n",
       "      <td>3.267712e+00</td>\n",
       "      <td>4.993943e+00</td>\n",
       "      <td>3.494198e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.621860e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.164536e+06</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>-2.549780e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.329073e+06</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.281130e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.493610e+06</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>7.390655e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.658146e+06</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>2.048800e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  atom_index_0  atom_index_1  scalar_coupling_constant\n",
       "count  4.658147e+06  4.658147e+06  4.658147e+06              4.658147e+06\n",
       "mean   2.329073e+06  1.335689e+01  5.883966e+00              1.592165e+01\n",
       "std    1.344691e+06  3.267712e+00  4.993943e+00              3.494198e+01\n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00             -3.621860e+01\n",
       "25%    1.164536e+06  1.100000e+01  2.000000e+00             -2.549780e-01\n",
       "50%    2.329073e+06  1.300000e+01  5.000000e+00              2.281130e+00\n",
       "75%    3.493610e+06  1.600000e+01  8.000000e+00              7.390655e+00\n",
       "max    4.658146e+06  2.800000e+01  2.800000e+01              2.048800e+02"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v3.3.0.json\", \"config\": {\"mark\": {\"tooltip\": null}, \"view\": {\"height\": 300, \"width\": 400}}, \"datasets\": {\"data-49f16c2b497caf509825d63ea0302313\": [{\"bins\": \"(-36.46, -24.164]\", \"scalar_coupling_constant\": 92}, {\"bins\": \"(-24.164, -12.109]\", \"scalar_coupling_constant\": 88965}, {\"bins\": \"(-12.109, -0.0538]\", \"scalar_coupling_constant\": 1171006}, {\"bins\": \"(-0.0538, 12.001]\", \"scalar_coupling_constant\": 2572510}, {\"bins\": \"(12.001, 24.056]\", \"scalar_coupling_constant\": 64140}, {\"bins\": \"(24.056, 36.111]\", \"scalar_coupling_constant\": 18227}, {\"bins\": \"(36.111, 48.166]\", \"scalar_coupling_constant\": 12397}, {\"bins\": \"(48.166, 60.221]\", \"scalar_coupling_constant\": 16153}, {\"bins\": \"(60.221, 72.276]\", \"scalar_coupling_constant\": 5657}, {\"bins\": \"(72.276, 84.331]\", \"scalar_coupling_constant\": 192254}, {\"bins\": \"(84.331, 96.386]\", \"scalar_coupling_constant\": 312829}, {\"bins\": \"(96.386, 108.441]\", \"scalar_coupling_constant\": 76866}, {\"bins\": \"(108.441, 120.495]\", \"scalar_coupling_constant\": 67526}, {\"bins\": \"(120.495, 132.55]\", \"scalar_coupling_constant\": 36547}, {\"bins\": \"(132.55, 144.605]\", \"scalar_coupling_constant\": 12719}, {\"bins\": \"(144.605, 156.66]\", \"scalar_coupling_constant\": 969}, {\"bins\": \"(156.66, 168.715]\", \"scalar_coupling_constant\": 29}, {\"bins\": \"(168.715, 180.77]\", \"scalar_coupling_constant\": 1}, {\"bins\": \"(180.77, 192.825]\", \"scalar_coupling_constant\": 640}, {\"bins\": \"(192.825, 204.88]\", \"scalar_coupling_constant\": 8620}], \"data-bd650d94dea89e9b7debcb25ac305b73\": [{\"atom_index_0\": 12, \"count\": 552263}, {\"atom_index_0\": 13, \"count\": 505229}, {\"atom_index_0\": 9, \"count\": 485891}, {\"atom_index_0\": 14, \"count\": 476556}, {\"atom_index_0\": 10, \"count\": 473624}, {\"atom_index_0\": 11, \"count\": 446131}, {\"atom_index_0\": 15, \"count\": 422173}, {\"atom_index_0\": 16, \"count\": 360001}, {\"atom_index_0\": 17, \"count\": 285575}, {\"atom_index_0\": 18, \"count\": 212841}, {\"atom_index_0\": 19, \"count\": 143722}, {\"atom_index_0\": 20, \"count\": 94149}, {\"atom_index_0\": 8, \"count\": 80296}, {\"atom_index_0\": 21, \"count\": 48795}, {\"atom_index_0\": 22, \"count\": 30594}, {\"atom_index_0\": 7, \"count\": 14104}, {\"atom_index_0\": 23, \"count\": 10245}, {\"atom_index_0\": 24, \"count\": 7108}, {\"atom_index_0\": 6, \"count\": 2825}, {\"atom_index_0\": 25, \"count\": 1439}, {\"atom_index_0\": 26, \"count\": 1050}, {\"atom_index_0\": 5, \"count\": 985}, {\"atom_index_0\": 3, \"count\": 829}, {\"atom_index_0\": 2, \"count\": 651}, {\"atom_index_0\": 4, \"count\": 635}, {\"atom_index_0\": 1, \"count\": 225}, {\"atom_index_0\": 27, \"count\": 116}, {\"atom_index_0\": 28, \"count\": 92}, {\"atom_index_0\": 0, \"count\": 3}], \"data-dc52daaae98aba4f767be711bfd461c0\": [{\"atom_index_1\": 2, \"count\": 537311}, {\"atom_index_1\": 1, \"count\": 517218}, {\"atom_index_1\": 3, \"count\": 458480}, {\"atom_index_1\": 4, \"count\": 439876}, {\"atom_index_1\": 5, \"count\": 415448}, {\"atom_index_1\": 6, \"count\": 392783}, {\"atom_index_1\": 7, \"count\": 359545}, {\"atom_index_1\": 0, \"count\": 322761}, {\"atom_index_1\": 8, \"count\": 248731}, {\"atom_index_1\": 11, \"count\": 136544}, {\"atom_index_1\": 12, \"count\": 117024}, {\"atom_index_1\": 13, \"count\": 108353}, {\"atom_index_1\": 14, \"count\": 95001}, {\"atom_index_1\": 15, \"count\": 89218}, {\"atom_index_1\": 10, \"count\": 85942}, {\"atom_index_1\": 16, \"count\": 81188}, {\"atom_index_1\": 17, \"count\": 70360}, {\"atom_index_1\": 18, \"count\": 57310}, {\"atom_index_1\": 19, \"count\": 41672}, {\"atom_index_1\": 20, \"count\": 30523}, {\"atom_index_1\": 21, \"count\": 16680}, {\"atom_index_1\": 9, \"count\": 14834}, {\"atom_index_1\": 22, \"count\": 12414}, {\"atom_index_1\": 23, \"count\": 4058}, {\"atom_index_1\": 24, \"count\": 3520}, {\"atom_index_1\": 26, \"count\": 629}, {\"atom_index_1\": 25, \"count\": 596}, {\"atom_index_1\": 28, \"count\": 76}, {\"atom_index_1\": 27, \"count\": 52}], \"data-ebdc32f1be979a9c6ddbf0cea0b853fe\": [{\"count\": 1510379, \"type\": \"3JHC\"}, {\"count\": 1140674, \"type\": \"2JHC\"}, {\"count\": 709416, \"type\": \"1JHC\"}, {\"count\": 590611, \"type\": \"3JHH\"}, {\"count\": 378036, \"type\": \"2JHH\"}, {\"count\": 166415, \"type\": \"3JHN\"}, {\"count\": 119253, \"type\": \"2JHN\"}, {\"count\": 43363, \"type\": \"1JHN\"}]}, \"vconcat\": [{\"hconcat\": [{\"data\": {\"name\": \"data-bd650d94dea89e9b7debcb25ac305b73\"}, \"encoding\": {\"tooltip\": [{\"field\": \"atom_index_0\", \"type\": \"quantitative\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"atom_index_0\"}, \"field\": \"atom_index_0\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector005\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of atom_index_0\", \"width\": 350}, {\"data\": {\"name\": \"data-dc52daaae98aba4f767be711bfd461c0\"}, \"encoding\": {\"tooltip\": [{\"field\": \"atom_index_1\", \"type\": \"quantitative\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"atom_index_1\"}, \"field\": \"atom_index_1\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector006\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of atom_index_1\", \"width\": 350}]}, {\"hconcat\": [{\"data\": {\"name\": \"data-ebdc32f1be979a9c6ddbf0cea0b853fe\"}, \"encoding\": {\"tooltip\": [{\"field\": \"type\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"type\"}, \"field\": \"type\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector007\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of type\", \"width\": 350}, {\"data\": {\"name\": \"data-49f16c2b497caf509825d63ea0302313\"}, \"encoding\": {\"tooltip\": [{\"field\": \"scalar_coupling_constant\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"scalar_coupling_constant\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector008\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"scalar_coupling_constant histogram\", \"width\": 400}]}]};     \n",
       "        vg_embed(\"#vega-chart\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_count = train['atom_index_0'].value_counts().reset_index().rename(columns={'atom_index_0': 'count', 'index': 'atom_index_0'})\n",
    "chart1 = alt.Chart(atom_count).mark_bar().encode(\n",
    "    x=alt.X(\"atom_index_0:N\", axis=alt.Axis(title='atom_index_0')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['atom_index_0', 'count']\n",
    ").properties(title=\"Counts of atom_index_0\", width=350).interactive()\n",
    "\n",
    "atom_count = train['atom_index_1'].value_counts().reset_index().rename(columns={'atom_index_1': 'count', 'index': 'atom_index_1'})\n",
    "chart2 = alt.Chart(atom_count).mark_bar().encode(\n",
    "    x=alt.X(\"atom_index_1:N\", axis=alt.Axis(title='atom_index_1')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['atom_index_1', 'count']\n",
    ").properties(title=\"Counts of atom_index_1\", width=350).interactive()\n",
    "\n",
    "type_count = train['type'].value_counts().reset_index().rename(columns={'type': 'count', 'index': 'type'})\n",
    "chart3 = alt.Chart(type_count).mark_bar().encode(\n",
    "    x=alt.X(\"type:N\", axis=alt.Axis(title='type')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['type', 'count']\n",
    ").properties(title=\"Counts of type\", width=350).interactive()\n",
    "\n",
    "hist_df = pd.cut(train['scalar_coupling_constant'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "chart4 = alt.Chart(hist_df).mark_bar().encode(\n",
    "    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n",
    "    y=alt.Y('scalar_coupling_constant:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['scalar_coupling_constant', 'bins']\n",
    ").properties(title=\"scalar_coupling_constant histogram\", width=400).interactive()\n",
    "\n",
    "\n",
    "render((chart1 | chart2) & (chart3 | chart4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (18, 6))\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.hist(train['scalar_coupling_constant'], bins=20);\n",
    "plt.title('Basic scalar_coupling_constant histogram');\n",
    "plt.subplot(1, 2, 2);\n",
    "sns.violinplot(x='type', y='scalar_coupling_constant', data=train);\n",
    "plt.title('Violinplot of scalar_coupling_constant by type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many interesting things here:\n",
    "- among first atoms there is a little number of atoms with index lower than 7 or higher than 24;\n",
    "- among second atoms there is a little number of atoms with index higher than 24. Also index with atom with index 9 in quite rare;\n",
    "- coupling types are unevenly distributed. There are 3 very popular, 3 quite rare and 2 with medium frequency;\n",
    "- target variable has a bimodal distribution;\n",
    "- different coupling types have really different values of target variable. Maybe it would make sense to build separate models for each of them;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting network graphs by type\n",
    "\n",
    "We have molecules, atom pairs, so this means data, which is interconnected. Network graphs should be useful to visualize such data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 12))\n",
    "for i, t in enumerate(train['type'].unique()):\n",
    "    train_type = train.loc[train['type'] == t]\n",
    "    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n",
    "    plt.subplot(2, 4, i + 1);\n",
    "    nx.draw(G, with_labels=True);\n",
    "    plt.title(f'Graph for type {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We can see that atom connections have different shapes for different types. Type 2JHH has an expecially unique scheme.\n",
    "Also we can see that some atoms are connected only to several other atoms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better network graphs\n",
    "But there is a little problem: as we saw earlier, there are atoms which are very rare, as a result graphs will be skewed due to them. Now I'll drop atoms for each type which are present in less then 1% of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 12))\n",
    "for i, t in enumerate(train['type'].unique()):\n",
    "    train_type = train.loc[train['type'] == t]\n",
    "    bad_atoms_0 = list(train_type['atom_index_0'].value_counts(normalize=True)[train_type['atom_index_0'].value_counts(normalize=True) < 0.01].index)\n",
    "    bad_atoms_1 = list(train_type['atom_index_1'].value_counts(normalize=True)[train_type['atom_index_1'].value_counts(normalize=True) < 0.01].index)\n",
    "    bad_atoms = list(set(bad_atoms_0 + bad_atoms_1))\n",
    "    train_type = train_type.loc[(train_type['atom_index_0'].isin(bad_atoms_0) == False) & (train_type['atom_index_1'].isin(bad_atoms_1) == False)]\n",
    "    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n",
    "    plt.subplot(2, 4, i + 1);\n",
    "    nx.draw(G, with_labels=True);\n",
    "    plt.title(f'Graph for type {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graphs are much more clear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "For now I'll use a basic approach to feature engineering.\n",
    "\n",
    "I'll use this useful kernel:\n",
    "https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create two new features: one will show the first character of the `type`, the second one will show other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])\n",
    "train['type_1'] = train['type'].apply(lambda x: x[1:])\n",
    "test['type_1'] = test['type'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, everyone uses this distance feature, let's have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.hist(train['dist'], bins=20);\n",
    "plt.title('Basic dist_speedup histogram');\n",
    "plt.subplot(1, 2, 2);\n",
    "sns.violinplot(x='type', y='dist', data=train);\n",
    "plt.title('Violinplot of dist_speedup by type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the values are quite different for different types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dist_to_type_mean'] = train['dist'] / train.groupby('type')['dist'].transform('mean')\n",
    "test['dist_to_type_mean'] = test['dist'] / test.groupby('type')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_0_mean'] = train['dist'] / train.groupby('type_0')['dist'].transform('mean')\n",
    "test['dist_to_type_0_mean'] = test['dist'] / test.groupby('type_0')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_1_mean'] = train['dist'] / train.groupby('type_1')['dist'].transform('mean')\n",
    "test['dist_to_type_1_mean'] = test['dist'] / test.groupby('type_1')['dist'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[f'molecule_type_dist_mean'] = train.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "test[f'molecule_type_dist_mean'] = test.groupby(['molecule_name', 'type'])['dist'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model\n",
    "\n",
    "I'll use the function for metric calculation from this kernel: https://www.kaggle.com/abhishek/competition-metric\n",
    "\n",
    "UPD: use faster metric from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "\n",
    "You can see the code in my utility script. Please note that to use this metric for calculation you need to pass value `group_mae` to parameter `eval_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['atom_0', 'atom_1', 'type_0', 'type_1', 'type']:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "    train[f] = lbl.transform(list(train[f].values))\n",
    "    test[f] = lbl.transform(list(test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'molecule_name', 'scalar_coupling_constant'], axis=1)\n",
    "y = train['scalar_coupling_constant']\n",
    "X_test = test.drop(['id', 'molecule_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 13,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "result_dict_lgb = train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=True,\n",
    "                                                      verbose=1000, early_stopping_rounds=200, n_estimators=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = result_dict_lgb['prediction']\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result_dict_lgb['prediction'], bins=40);\n",
    "plt.title('Distribution of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot oof predictions vs target\n",
    "\n",
    "I use the code from benchmark kernel: https://www.kaggle.com/inversion/atomic-distance-benchmark\n",
    "\n",
    "Notice that while using `LabelEncoder` on categorical variables, `type` was the last feature, so `lbl` contains encodings for it and we can inverse_transform `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(y)\n",
    "plot_data.index.name = 'id'\n",
    "plot_data['yhat'] = result_dict_lgb['oof']\n",
    "plot_data['type'] = lbl.inverse_transform(X['type'])\n",
    "\n",
    "def plot_oof_preds(ctype, llim, ulim):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.scatterplot(x='scalar_coupling_constant',y='yhat',\n",
    "                        data=plot_data.loc[plot_data['type']==ctype,\n",
    "                        ['scalar_coupling_constant', 'yhat']]);\n",
    "        plt.xlim((llim, ulim))\n",
    "        plt.ylim((llim, ulim))\n",
    "        plt.plot([llim, ulim], [llim, ulim])\n",
    "        plt.xlabel('scalar_coupling_constant')\n",
    "        plt.ylabel('predicted')\n",
    "        plt.title(f'{ctype}', fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "plot_oof_preds('1JHC', 0, 250)\n",
    "plot_oof_preds('1JHN', 0, 100)\n",
    "plot_oof_preds('2JHC', -50, 50)\n",
    "plot_oof_preds('2JHH', -50, 50)\n",
    "plot_oof_preds('2JHN', -25, 25)\n",
    "plot_oof_preds('3JHC', -25, 100)\n",
    "plot_oof_preds('3JHH', -20, 20)\n",
    "plot_oof_preds('3JHN', -15, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some types has better predictions than others. Maybe we should focus on improving models for these types?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
